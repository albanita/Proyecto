{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3XrDOihm4ni",
        "colab_type": "code",
        "outputId": "21009b8c-e713-4c9d-8c7c-1e4c8a29514a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3842
        }
      },
      "source": [
        "!wget https://github.com/albanita/Proyecto/blob/master/dataset.zip?raw=true -O dataset.zip\n",
        "!unzip dataset.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-04 12:12:24--  https://github.com/albanita/Proyecto/blob/master/dataset.zip?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/albanita/Proyecto/raw/master/dataset.zip [following]\n",
            "--2019-05-04 12:12:24--  https://github.com/albanita/Proyecto/raw/master/dataset.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/albanita/Proyecto/master/dataset.zip [following]\n",
            "--2019-05-04 12:12:25--  https://raw.githubusercontent.com/albanita/Proyecto/master/dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7141022 (6.8M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   6.81M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-05-04 12:12:25 (82.3 MB/s) - ‘dataset.zip’ saved [7141022/7141022]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "replace dataset/ground-truth/100098.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: dataset/ground-truth/100098.png  \n",
            "  inflating: dataset/ground-truth/101027.png  \n",
            "  inflating: dataset/ground-truth/103006.png  \n",
            "  inflating: dataset/ground-truth/103029.png  \n",
            "  inflating: dataset/ground-truth/104010.png  \n",
            "  inflating: dataset/ground-truth/105027.png  \n",
            "  inflating: dataset/ground-truth/106005.png  \n",
            "  inflating: dataset/ground-truth/106024.png  \n",
            "  inflating: dataset/ground-truth/106025.png  \n",
            "  inflating: dataset/ground-truth/106047.png  \n",
            "  inflating: dataset/ground-truth/107072.png  \n",
            "  inflating: dataset/ground-truth/108004.png  \n",
            "  inflating: dataset/ground-truth/108041.png  \n",
            "  inflating: dataset/ground-truth/108069.png  \n",
            "  inflating: dataset/ground-truth/108070.png  \n",
            "  inflating: dataset/ground-truth/112056.png  \n",
            "  inflating: dataset/ground-truth/112082.png  \n",
            "  inflating: dataset/ground-truth/113016.png  \n",
            "  inflating: dataset/ground-truth/113044.png  \n",
            " extracting: dataset/ground-truth/12003.png  \n",
            "  inflating: dataset/ground-truth/124084.png  \n",
            "  inflating: dataset/ground-truth/126007.png  \n",
            "  inflating: dataset/ground-truth/130014.png  \n",
            "  inflating: dataset/ground-truth/130026.png  \n",
            "  inflating: dataset/ground-truth/134008.png  \n",
            "  inflating: dataset/ground-truth/135037.png  \n",
            "  inflating: dataset/ground-truth/135069.png  \n",
            "  inflating: dataset/ground-truth/143090.png  \n",
            "  inflating: dataset/ground-truth/15062.png  \n",
            "  inflating: dataset/ground-truth/157032.png  \n",
            "  inflating: dataset/ground-truth/159091.png  \n",
            "  inflating: dataset/ground-truth/160068.png  \n",
            "  inflating: dataset/ground-truth/164046.png  \n",
            "  inflating: dataset/ground-truth/168084.png  \n",
            "  inflating: dataset/ground-truth/173036.png  \n",
            "  inflating: dataset/ground-truth/181021.png  \n",
            "  inflating: dataset/ground-truth/183087.png  \n",
            "  inflating: dataset/ground-truth/187029.png  \n",
            "  inflating: dataset/ground-truth/187083.png  \n",
            "  inflating: dataset/ground-truth/189011.png  \n",
            "  inflating: dataset/ground-truth/189080.png  \n",
            "  inflating: dataset/ground-truth/207056.png  \n",
            "  inflating: dataset/ground-truth/208078.png  \n",
            "  inflating: dataset/ground-truth/227092.png  \n",
            "  inflating: dataset/ground-truth/228076.png  \n",
            "  inflating: dataset/ground-truth/23084.png  \n",
            "  inflating: dataset/ground-truth/232076.png  \n",
            "  inflating: dataset/ground-truth/238011.png  \n",
            "  inflating: dataset/ground-truth/246009.png  \n",
            "  inflating: dataset/ground-truth/247003.png  \n",
            "  inflating: dataset/ground-truth/247085.png  \n",
            "  inflating: dataset/ground-truth/253092.png  \n",
            "  inflating: dataset/ground-truth/26031.png  \n",
            "  inflating: dataset/ground-truth/285022.png  \n",
            "  inflating: dataset/ground-truth/29030.png  \n",
            "  inflating: dataset/ground-truth/291000.png  \n",
            "  inflating: dataset/ground-truth/296059.png  \n",
            "  inflating: dataset/ground-truth/299086.png  \n",
            "  inflating: dataset/ground-truth/299091.png  \n",
            "  inflating: dataset/ground-truth/304034.png  \n",
            "  inflating: dataset/ground-truth/304074.png  \n",
            "  inflating: dataset/ground-truth/3063.png  \n",
            "  inflating: dataset/ground-truth/309004.png  \n",
            "  inflating: dataset/ground-truth/3096.png  \n",
            "  inflating: dataset/ground-truth/310007.png  \n",
            "  inflating: dataset/ground-truth/323016.png  \n",
            " extracting: dataset/ground-truth/35010.png  \n",
            " extracting: dataset/ground-truth/35058.png  \n",
            "  inflating: dataset/ground-truth/35070.png  \n",
            "  inflating: dataset/ground-truth/353013.png  \n",
            " extracting: dataset/ground-truth/368037.png  \n",
            "  inflating: dataset/ground-truth/41006.png  \n",
            "  inflating: dataset/ground-truth/41025.png  \n",
            "  inflating: dataset/ground-truth/41029.png  \n",
            "  inflating: dataset/ground-truth/41033.png  \n",
            "  inflating: dataset/ground-truth/41069.png  \n",
            " extracting: dataset/ground-truth/41085.png  \n",
            "  inflating: dataset/ground-truth/41096.png  \n",
            "  inflating: dataset/ground-truth/42044.png  \n",
            "  inflating: dataset/ground-truth/42049.png  \n",
            "  inflating: dataset/ground-truth/42078.png  \n",
            "  inflating: dataset/ground-truth/43033.png  \n",
            "  inflating: dataset/ground-truth/43051.png  \n",
            "  inflating: dataset/ground-truth/43074.png  \n",
            "  inflating: dataset/ground-truth/46076.png  \n",
            "  inflating: dataset/ground-truth/48017.png  \n",
            " extracting: dataset/ground-truth/51084.png  \n",
            "  inflating: dataset/ground-truth/6046.png  \n",
            "  inflating: dataset/ground-truth/61060.png  \n",
            "  inflating: dataset/ground-truth/64061.png  \n",
            " extracting: dataset/ground-truth/69020.png  \n",
            "  inflating: dataset/ground-truth/69022.png  \n",
            "  inflating: dataset/ground-truth/69040.png  \n",
            "  inflating: dataset/ground-truth/70011.png  \n",
            "  inflating: dataset/ground-truth/80090.png  \n",
            "  inflating: dataset/ground-truth/80099.png  \n",
            "  inflating: dataset/ground-truth/8068.png  \n",
            "  inflating: dataset/ground-truth/81095.png  \n",
            "  inflating: dataset/ground-truth/87015.png  \n",
            "  inflating: dataset/ground-truth/87046.png  \n",
            "  inflating: dataset/image/100098.jpg  \n",
            "  inflating: dataset/image/101027.jpg  \n",
            "  inflating: dataset/image/103006.jpg  \n",
            "  inflating: dataset/image/103029.jpg  \n",
            "  inflating: dataset/image/104010.jpg  \n",
            "  inflating: dataset/image/105027.jpg  \n",
            "  inflating: dataset/image/106005.jpg  \n",
            "  inflating: dataset/image/106024.jpg  \n",
            "  inflating: dataset/image/106025.jpg  \n",
            "  inflating: dataset/image/106047.jpg  \n",
            "  inflating: dataset/image/107072.jpg  \n",
            "  inflating: dataset/image/108004.jpg  \n",
            "  inflating: dataset/image/108041.jpg  \n",
            "  inflating: dataset/image/108069.jpg  \n",
            "  inflating: dataset/image/108070.jpg  \n",
            "  inflating: dataset/image/112056.jpg  \n",
            "  inflating: dataset/image/112082.jpg  \n",
            "  inflating: dataset/image/113016.jpg  \n",
            "  inflating: dataset/image/113044.jpg  \n",
            "  inflating: dataset/image/12003.jpg  \n",
            "  inflating: dataset/image/124084.jpg  \n",
            "  inflating: dataset/image/126007.jpg  \n",
            "  inflating: dataset/image/130014.jpg  \n",
            "  inflating: dataset/image/130026.jpg  \n",
            "  inflating: dataset/image/134008.jpg  \n",
            "  inflating: dataset/image/135037.jpg  \n",
            "  inflating: dataset/image/135069.jpg  \n",
            "  inflating: dataset/image/143090.jpg  \n",
            "  inflating: dataset/image/15062.jpg  \n",
            "  inflating: dataset/image/157032.jpg  \n",
            "  inflating: dataset/image/159091.jpg  \n",
            "  inflating: dataset/image/160068.jpg  \n",
            "  inflating: dataset/image/164046.jpg  \n",
            "  inflating: dataset/image/168084.jpg  \n",
            "  inflating: dataset/image/173036.jpg  \n",
            "  inflating: dataset/image/181021.jpg  \n",
            "  inflating: dataset/image/183087.jpg  \n",
            "  inflating: dataset/image/187029.jpg  \n",
            "  inflating: dataset/image/187083.jpg  \n",
            "  inflating: dataset/image/189011.jpg  \n",
            "  inflating: dataset/image/189080.jpg  \n",
            "  inflating: dataset/image/207056.jpg  \n",
            "  inflating: dataset/image/208078.jpg  \n",
            "  inflating: dataset/image/227092.jpg  \n",
            "  inflating: dataset/image/228076.jpg  \n",
            "  inflating: dataset/image/23084.jpg  \n",
            "  inflating: dataset/image/232076.jpg  \n",
            "  inflating: dataset/image/238011.jpg  \n",
            "  inflating: dataset/image/246009.jpg  \n",
            "  inflating: dataset/image/247003.jpg  \n",
            "  inflating: dataset/image/247085.jpg  \n",
            "  inflating: dataset/image/253092.jpg  \n",
            "  inflating: dataset/image/26031.jpg  \n",
            "  inflating: dataset/image/285022.jpg  \n",
            "  inflating: dataset/image/29030.jpg  \n",
            "  inflating: dataset/image/291000.jpg  \n",
            "  inflating: dataset/image/296059.jpg  \n",
            "  inflating: dataset/image/299086.jpg  \n",
            "  inflating: dataset/image/299091.jpg  \n",
            "  inflating: dataset/image/304034.jpg  \n",
            "  inflating: dataset/image/304074.jpg  \n",
            "  inflating: dataset/image/3063.jpg  \n",
            "  inflating: dataset/image/309004.jpg  \n",
            "  inflating: dataset/image/3096.jpg  \n",
            "  inflating: dataset/image/310007.jpg  \n",
            "  inflating: dataset/image/323016.jpg  \n",
            "  inflating: dataset/image/35010.jpg  \n",
            "  inflating: dataset/image/35058.jpg  \n",
            "  inflating: dataset/image/35070.jpg  \n",
            "  inflating: dataset/image/353013.jpg  \n",
            "  inflating: dataset/image/368037.jpg  \n",
            "  inflating: dataset/image/41006.jpg  \n",
            "  inflating: dataset/image/41025.jpg  \n",
            "  inflating: dataset/image/41029.jpg  \n",
            "  inflating: dataset/image/41033.jpg  \n",
            "  inflating: dataset/image/41069.jpg  \n",
            "  inflating: dataset/image/41085.jpg  \n",
            "  inflating: dataset/image/41096.jpg  \n",
            "  inflating: dataset/image/42044.jpg  \n",
            "  inflating: dataset/image/42049.jpg  \n",
            "  inflating: dataset/image/42078.jpg  \n",
            "  inflating: dataset/image/43033.jpg  \n",
            "  inflating: dataset/image/43051.jpg  \n",
            "  inflating: dataset/image/43074.jpg  \n",
            "  inflating: dataset/image/46076.jpg  \n",
            "  inflating: dataset/image/48017.jpg  \n",
            "  inflating: dataset/image/51084.jpg  \n",
            "  inflating: dataset/image/6046.jpg  \n",
            "  inflating: dataset/image/61060.jpg  \n",
            "  inflating: dataset/image/64061.jpg  \n",
            "  inflating: dataset/image/69020.jpg  \n",
            "  inflating: dataset/image/69022.jpg  \n",
            "  inflating: dataset/image/69040.jpg  \n",
            "  inflating: dataset/image/70011.jpg  \n",
            "  inflating: dataset/image/80090.jpg  \n",
            "  inflating: dataset/image/80099.jpg  \n",
            "  inflating: dataset/image/8068.jpg  \n",
            "  inflating: dataset/image/81095.jpg  \n",
            "  inflating: dataset/image/87015.jpg  \n",
            "  inflating: dataset/image/87046.jpg  \n",
            "  inflating: dataset/read me.txt     \n",
            "  inflating: dataset/result.txt      \n",
            "  inflating: dataset/test/23084.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpUsRQPxwwVT",
        "colab_type": "text"
      },
      "source": [
        "# Dailed Back Propagation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WE2Bdy6m_F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from numpy import float32\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.set_random_seed(678)\n",
        "\n",
        "# Activation Functions - however there was no indication in the original paper\n",
        "def tf_Relu(x): return tf.nn.relu(x)\n",
        "def d_tf_Relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
        "\n",
        "def tf_log(x): return tf.sigmoid(x)\n",
        "def d_tf_log(x): return tf_log(x) * (1.0 - tf.log(x))\n",
        "\n",
        "def tf_tanh(x): return tf.tanh(x)\n",
        "def d_tf_tanh(x): return 1.0 - tf.square(tf_tanh(x))\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    X = np.asarray(dict[b'data'].T).astype(\"uint8\")\n",
        "    Yraw = np.asarray(dict[b'labels'])\n",
        "    Y = np.zeros((10,10000))\n",
        "    for i in range(10000):\n",
        "        Y[Yraw[i],i] = 1\n",
        "    names = np.asarray(dict[b'filenames'])\n",
        "    return X,Y,names\n",
        "\n",
        "# make class \n",
        "class CNNLayer():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c,act,d_act,):\n",
        "        \n",
        "        self.w = tf.Variable(tf.truncated_normal([ker,ker,in_c,out_c],stddev=0.005))\n",
        "        self.act,self.d_act = act,d_act\n",
        "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "\n",
        "    def feedforward(self,input,stride=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient,stride=1):\n",
        "        grad_part_1 = gradient\n",
        "        grad_part_2 = self.d_act(self.layer)\n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.nn.conv2d_backprop_filter(\n",
        "            input = grad_part_3,filter_sizes = self.w.shape,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        grad_pass  = tf.nn.conv2d_backprop_input(\n",
        "            input_sizes=[batch_size] + list(self.input.shape[1:]),filter = self.w ,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "\n",
        "        update_w.append(\n",
        "            tf.assign( self.m,self.m*beta_1 + (1-beta_1) * grad   )\n",
        "        )\n",
        "        update_w.append(\n",
        "            tf.assign( self.v,self.v*beta_2 + (1-beta_2) * grad ** 2   )\n",
        "        )\n",
        "\n",
        "        m_hat = self.m / (1-beta_1)\n",
        "        v_hat = self.v / (1-beta_2)\n",
        "        adam_middel = learing_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat))))\n",
        "\n",
        "        return grad_pass,update_w\n",
        "\n",
        "\n",
        "data_location = \"./big_image/\"\n",
        "data_array = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():  # check whether the file's DICOM\n",
        "            data_array.append(os.path.join(dirName,filename))\n",
        "\n",
        "X = np.zeros(shape=(100,80,80,3))\n",
        "\n",
        "for file_index in range(len(data_array)):\n",
        "    X[file_index,:,:]   = imresize(imread(data_array[file_index],mode='RGB'),(80,80))\n",
        "\n",
        "X[:,:,:,0] = (X[:,:,:,0]-X[:,:,:,0].min(axis=0))/(X[:,:,:,0].max(axis=0)-X[:,:,:,0].min(axis=0))\n",
        "X[:,:,:,1] = (X[:,:,:,1]-X[:,:,:,1].min(axis=0))/(X[:,:,:,1].max(axis=0)-X[:,:,:,1].min(axis=0))\n",
        "X[:,:,:,2] = (X[:,:,:,2]-X[:,:,:,2].min(axis=0))/(X[:,:,:,2].max(axis=0)-X[:,:,:,2].min(axis=0))\n",
        "\n",
        "X = shuffle(X)\n",
        "s_images = X[:50,:,:,:]\n",
        "c_images = X[50:,:,:,:]\n",
        "\n",
        "# hyper\n",
        "num_epoch = 1000\n",
        "num_epoch = 30000\n",
        "\n",
        "learing_rate = 0.0001\n",
        "batch_size = 10\n",
        "\n",
        "networ_beta = 1.0\n",
        "\n",
        "beta_1,beta_2 = 0.9,0.999\n",
        "adam_e = 1e-8\n",
        "\n",
        "proportion_rate = 0.6\n",
        "decay_rate = 0.9\n",
        "\n",
        "# init class\n",
        "\n",
        "#preparation network\n",
        "#red de preparación\n",
        "prep_net1 = CNNLayer(3,3,50,tf_Relu,d_tf_Relu)\n",
        "prep_net2 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net3 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net4 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net5 = CNNLayer(3,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "#hide network\n",
        "#red de ocultación\n",
        "hide_net1 = CNNLayer(4,6,50,tf_Relu,d_tf_Relu)\n",
        "hide_net2 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net3 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net4 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net5 = CNNLayer(4,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "#reveal network\n",
        "#red de revelación\n",
        "reve_net1 = CNNLayer(5,3,50,tf_Relu,d_tf_Relu)\n",
        "reve_net2 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net3 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net4 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net5 = CNNLayer(5,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "# make graph\n",
        "Secret = tf.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "Cover = tf.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "\n",
        "iter_variable_dil = tf.placeholder(tf.float32, shape=())\n",
        "decay_propotoin_rate = proportion_rate / (1 + decay_rate * iter_variable_dil)\n",
        "\n",
        "#preparation network and feedFoward operation\n",
        "#red de preparacion con su operacion feedFoward\n",
        "prep_layer1 = prep_net1.feedforward(Secret)\n",
        "prep_layer2 = prep_net2.feedforward(prep_layer1)\n",
        "prep_layer3 = prep_net3.feedforward(prep_layer2)\n",
        "prep_layer4 = prep_net4.feedforward(prep_layer3)\n",
        "prep_layer5 = prep_net5.feedforward(prep_layer4)\n",
        "\n",
        "#hiding network and feedFoward operation\n",
        "#red de ocultacion con su operacion feedFoward\n",
        "hide_Input = tf.concat([Cover,prep_layer5],axis=3)\n",
        "hide_layer1 = hide_net1.feedforward(hide_Input)\n",
        "hide_layer2 = hide_net2.feedforward(hide_layer1)\n",
        "hide_layer3 = hide_net3.feedforward(hide_layer2)\n",
        "hide_layer4 = hide_net4.feedforward(hide_layer3)\n",
        "hide_layer5 = hide_net5.feedforward(hide_layer4)\n",
        "\n",
        "#revealing network and feedFoward operation\n",
        "#red de revelacion con su operacion feedFoward\n",
        "reve_layer1 = reve_net1.feedforward(hide_layer5)\n",
        "reve_layer2 = reve_net2.feedforward(reve_layer1)\n",
        "reve_layer3 = reve_net3.feedforward(reve_layer2)\n",
        "reve_layer4 = reve_net4.feedforward(reve_layer3)\n",
        "reve_layer5 = reve_net5.feedforward(reve_layer4)\n",
        "\n",
        "cost_1 = tf.reduce_mean(tf.square(hide_layer5 - Cover))*0.5\n",
        "cost_2 = tf.reduce_mean(tf.square(reve_layer5 - Secret)) *0.5\n",
        "\n",
        "# --- auto train ---\n",
        "# auto_train = tf.train.AdamOptimizer(learning_rate=learing_rate).minimize(cost_1+cost_2)\n",
        "\n",
        "# entrenamiento de la red\n",
        "reve_net_grad5,reve_net_grad5w = reve_net5.backprop(reve_layer5-Secret)\n",
        "reve_net_grad4,reve_net_grad4w = reve_net4.backprop(reve_net_grad5)\n",
        "reve_net_grad3,reve_net_grad3w = reve_net3.backprop(reve_net_grad4 + decay_propotoin_rate * (reve_net_grad5))\n",
        "reve_net_grad2,reve_net_grad2w = reve_net2.backprop(reve_net_grad3+ decay_propotoin_rate * (reve_net_grad5 + reve_net_grad4))\n",
        "reve_net_grad1,reve_net_grad1w = reve_net1.backprop(reve_net_grad2+ decay_propotoin_rate * (reve_net_grad5 + reve_net_grad4 + reve_net_grad3))\n",
        "\n",
        "hide_net_grad5,hide_net_grad5w = hide_net5.backprop( (hide_layer5-Cover) + reve_net_grad1 )\n",
        "hide_net_grad4,hide_net_grad4w = hide_net4.backprop(hide_net_grad5)\n",
        "hide_net_grad3,hide_net_grad3w = hide_net3.backprop(hide_net_grad4+ decay_propotoin_rate *(hide_net_grad5 ))\n",
        "hide_net_grad2,hide_net_grad2w = hide_net2.backprop(hide_net_grad3+ decay_propotoin_rate *(hide_net_grad5 + hide_net_grad4))\n",
        "hide_net_grad1,hide_net_grad1w = hide_net1.backprop(hide_net_grad2+ decay_propotoin_rate *(hide_net_grad5 + hide_net_grad4 + hide_net_grad3))\n",
        "\n",
        "prep_net_Input = hide_net_grad1[:,:,:,3:]\n",
        "prep_net_grad5,prep_net_grad5w = prep_net5.backprop(prep_net_Input)\n",
        "prep_net_grad4,prep_net_grad4w = prep_net4.backprop(prep_net_grad5)\n",
        "prep_net_grad3,prep_net_grad3w = prep_net3.backprop(prep_net_grad4+ decay_propotoin_rate *(prep_net_grad5))\n",
        "prep_net_grad2,prep_net_grad2w = prep_net2.backprop(prep_net_grad3+ decay_propotoin_rate *(prep_net_grad5 +prep_net_grad4 ))\n",
        "prep_net_grad1,prep_net_grad1w = prep_net1.backprop(prep_net_grad2+ decay_propotoin_rate *(prep_net_grad5 + prep_net_grad4 +prep_net_grad3 ))\n",
        "\n",
        "grad_update = reve_net_grad5w + reve_net_grad4w + reve_net_grad3w + reve_net_grad2w + reve_net_grad1w + \\\n",
        "                hide_net_grad5w + hide_net_grad4w + hide_net_grad3w + hide_net_grad2w + hide_net_grad1w + \\\n",
        "                prep_net_grad5w + prep_net_grad4w + prep_net_grad3w + prep_net_grad2w + prep_net_grad1w\n",
        "\n",
        "\n",
        "# empieza a usar lo que ha entrenado ????\n",
        "# start the session\n",
        "with tf.Session() as sess : \n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        for current_batch_index in range(0,len(s_images),batch_size):\n",
        "            current_batch_s = s_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_batch_c = c_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            sess_results = sess.run([cost_1,cost_2,grad_update],feed_dict={Secret:current_batch_s,Cover:current_batch_c,iter_variable_dil:iter})\n",
        "            print(\"Iter: \",iter, ' cost 1: ',sess_results[0],' cost 2: ',sess_results[1],end='\\r')\n",
        "\n",
        "        if iter % 100 == 0 :\n",
        "            random_data_index = np.random.randint(len(s_images))\n",
        "            current_batch_s = np.expand_dims(s_images[random_data_index,:,:,:],0)\n",
        "            current_batch_c = np.expand_dims(c_images[random_data_index,:,:,:],0)\n",
        "            sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' Secret')\n",
        "            plt.savefig('images/epoch_'+str(iter)+\"a_secret.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' cover')\n",
        "            plt.savefig('images/epoch_'+str(iter)+\"b_cover.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' prep image')\n",
        "            plt.savefig('images/epoch_'+str(iter)+\"c_prep_images.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+\" Hidden Image \")\n",
        "            plt.savefig('images/epoch_'+str(iter)+\"d_hidden_image.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "            plt.title('epoch_'+str(iter)+\" Reveal  Image\")\n",
        "            plt.savefig('images/epoch_'+str(iter)+\"e_reveal_images.png\")\n",
        "\n",
        "            plt.close('all')\n",
        "            print('\\n--------------------\\n')\n",
        "\n",
        "        if iter == num_epoch-1:\n",
        "            \n",
        "            for final in range(len(s_images)):\n",
        "                current_batch_s = np.expand_dims(s_images[final,:,:,:],0)\n",
        "                current_batch_c = np.expand_dims(c_images[final,:,:,:],0)\n",
        "                sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' Secret')\n",
        "                plt.savefig('gif/epoch_'+str(final)+\"a_secret.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' cover')\n",
        "                plt.savefig('gif/epoch_'+str(final)+\"b_cover.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' prep image')\n",
        "                plt.savefig('gif/epoch_'+str(final)+\"c_prep_images.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+\" Hidden Image \")\n",
        "                plt.savefig('gif/epoch_'+str(final)+\"d_hidden_image.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.axis('off')\n",
        "                plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "                plt.title('epoch_'+str(final)+\" Reveal  Image\")\n",
        "                plt.savefig('gif/epoch_'+str(final)+\"e_reveal_images.png\")\n",
        "\n",
        "                plt.close('all')\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoNTJXjjxdyz",
        "colab_type": "text"
      },
      "source": [
        "# Auto differentiation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UqpNPxHxhK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install unzip\n",
        "# !wget -O data.zip http://www.ntu.edu.sg/home/asjfcai/Benchmark_Website/Semantic%20dataset100.zip\n",
        "# !unzip data.zip\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from numpy import float32\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.set_random_seed(678)\n",
        "\n",
        "# Activation Functions - however there was no indication in the original paper\n",
        "def tf_Relu(x): return tf.nn.relu(x)\n",
        "def d_tf_Relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
        "\n",
        "def tf_log(x): return tf.sigmoid(x)\n",
        "def d_tf_log(x): return tf_log(x) * (1.0 - tf.log(x))\n",
        "\n",
        "def tf_tanh(x): return tf.tanh(x)\n",
        "def d_tf_tanh(x): return 1.0 - tf.square(tf_tanh(x))\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    X = np.asarray(dict[b'data'].T).astype(\"uint8\")\n",
        "    Yraw = np.asarray(dict[b'labels'])\n",
        "    Y = np.zeros((10,10000))\n",
        "    for i in range(10000):\n",
        "        Y[Yraw[i],i] = 1\n",
        "    names = np.asarray(dict[b'filenames'])\n",
        "    return X,Y,names\n",
        "\n",
        "# make class \n",
        "class CNNLayer():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c,act,d_act,):\n",
        "        \n",
        "        self.w = tf.Variable(tf.truncated_normal([ker,ker,in_c,out_c],stddev=0.005))\n",
        "        self.act,self.d_act = act,d_act\n",
        "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "\n",
        "    def feedforward(self,input,stride=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient,stride=1):\n",
        "        grad_part_1 = gradient\n",
        "        grad_part_2 = self.d_act(self.layer)\n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.nn.conv2d_backprop_filter(\n",
        "            input = grad_part_3,filter_sizes = self.w.shape,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        grad_pass  = tf.nn.conv2d_backprop_input(\n",
        "            input_sizes=[batch_size] + list(self.input.shape[1:]),filter = self.w ,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "\n",
        "        update_w.append(\n",
        "            tf.assign( self.m,self.m*beta_1 + (1-beta_1) * grad   )\n",
        "        )\n",
        "        update_w.append(\n",
        "            tf.assign( self.v,self.v*beta_2 + (1-beta_2) * grad ** 2   )\n",
        "        )\n",
        "\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat))))\n",
        "\n",
        "        return grad_pass,update_w\n",
        "\n",
        "data_location = \"./dataset/image/\"\n",
        "data_array = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():  # check whether the file's DICOM\n",
        "            data_array.append(os.path.join(dirName,filename))\n",
        "\n",
        "X = np.zeros(shape=(100,80,80,3))\n",
        "\n",
        "for file_index in range(len(data_array)):\n",
        "    X[file_index,:,:]   = imresize(imread(data_array[file_index],mode='RGB'),(80,80))\n",
        "\n",
        "X[:,:,:,0] = (X[:,:,:,0]-X[:,:,:,0].min(axis=0))/(X[:,:,:,0].max(axis=0)-X[:,:,:,0].min(axis=0))\n",
        "X[:,:,:,1] = (X[:,:,:,1]-X[:,:,:,1].min(axis=0))/(X[:,:,:,1].max(axis=0)-X[:,:,:,1].min(axis=0))\n",
        "X[:,:,:,2] = (X[:,:,:,2]-X[:,:,:,2].min(axis=0))/(X[:,:,:,2].max(axis=0)-X[:,:,:,2].min(axis=0))\n",
        "\n",
        "X = shuffle(X)\n",
        "s_images = X[:50,:,:,:]\n",
        "c_images = X[50:,:,:,:]\n",
        "\n",
        "# hyper\n",
        "num_epoch = 1000\n",
        "num_epoch = 10000\n",
        "\n",
        "learing_rate = 0.0008\n",
        "batch_size = 10\n",
        "\n",
        "networ_beta = 1.0\n",
        "\n",
        "beta_1,beta_2 = 0.9,0.999\n",
        "adam_e = 1e-8\n",
        "\n",
        "# init class\n",
        "prep_net1 = CNNLayer(3,3,50,tf_Relu,d_tf_Relu)\n",
        "prep_net2 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net3 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net4 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net5 = CNNLayer(3,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "hide_net1 = CNNLayer(4,6,50,tf_Relu,d_tf_Relu)\n",
        "hide_net2 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net3 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net4 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net5 = CNNLayer(4,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "reve_net1 = CNNLayer(5,3,50,tf_Relu,d_tf_Relu)\n",
        "reve_net2 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net3 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net4 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net5 = CNNLayer(5,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "# make graph\n",
        "Secret = tf.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "Cover = tf.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "\n",
        "prep_layer1 = prep_net1.feedforward(Secret)\n",
        "prep_layer2 = prep_net2.feedforward(prep_layer1)\n",
        "prep_layer3 = prep_net3.feedforward(prep_layer2)\n",
        "prep_layer4 = prep_net4.feedforward(prep_layer3)\n",
        "prep_layer5 = prep_net5.feedforward(prep_layer4)\n",
        "\n",
        "hide_Input = tf.concat([Cover,prep_layer5],axis=3)\n",
        "hide_layer1 = hide_net1.feedforward(hide_Input)\n",
        "hide_layer2 = hide_net2.feedforward(hide_layer1)\n",
        "hide_layer3 = hide_net3.feedforward(hide_layer2)\n",
        "hide_layer4 = hide_net4.feedforward(hide_layer3)\n",
        "hide_layer5 = hide_net5.feedforward(hide_layer4)\n",
        "\n",
        "reve_layer1 = reve_net1.feedforward(hide_layer5)\n",
        "reve_layer2 = reve_net2.feedforward(reve_layer1)\n",
        "reve_layer3 = reve_net3.feedforward(reve_layer2)\n",
        "reve_layer4 = reve_net4.feedforward(reve_layer3)\n",
        "reve_layer5 = reve_net5.feedforward(reve_layer4)\n",
        "\n",
        "cost_1 = tf.reduce_mean(tf.square(hide_layer5 - Cover))*0.5\n",
        "cost_2 = tf.reduce_mean(tf.square(reve_layer5 - Secret)) *0.5\n",
        "\n",
        "# --- auto train ---\n",
        "auto_train = tf.train.AdamOptimizer(learning_rate=learing_rate).minimize(cost_1+cost_2)\n",
        "\n",
        "\n",
        "# start the session\n",
        "with tf.Session() as sess : \n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        for current_batch_index in range(0,len(s_images),batch_size):\n",
        "            current_batch_s = s_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_batch_c = c_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            sess_results = sess.run([cost_1,cost_2,auto_train],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "            print(\"Iter: \",iter, ' cost 1: ',sess_results[0],' cost 2: ',sess_results[1],end='\\r')\n",
        "\n",
        "        if iter % 500 == 0 :\n",
        "            random_data_index = np.random.randint(len(s_images))\n",
        "            current_batch_s = np.expand_dims(s_images[random_data_index,:,:,:],0)\n",
        "            current_batch_c = np.expand_dims(c_images[random_data_index,:,:,:],0)\n",
        "            sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' Secret')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' cover')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' prep image')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+\" Hidden Image \")\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "            plt.title('epoch_'+str(iter)+\" Reveal  Image\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.close('all')\n",
        "            print('\\n--------------------\\n')\n",
        "\n",
        "        if iter == num_epoch-1:\n",
        "            \n",
        "            for final in range(len(s_images)):\n",
        "                current_batch_s = np.expand_dims(s_images[final,:,:,:],0)\n",
        "                current_batch_c = np.expand_dims(c_images[final,:,:,:],0)\n",
        "                sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' Secret')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' cover')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' prep image')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+\" Hidden Image \")\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.axis('off')\n",
        "                plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "                plt.title('epoch_'+str(final)+\" Reveal  Image\")\n",
        "                plt.show()\n",
        "\n",
        "                plt.close('all')\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}